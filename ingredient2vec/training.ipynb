{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 17:22:02.802390: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-09 17:22:03.152734: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-09 17:22:03.152753: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-09 17:22:04.580515: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-09 17:22:04.580655: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-09 17:22:04.580666: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dot\n",
    "from keras.layers.core import Dense, Reshape\n",
    "import keras\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import tqdm\n",
    "import pickle\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.load(\"vocab.npy\", allow_pickle=True)\n",
    "X = np.load(\"training_data_X.pickle\", allow_pickle=True)\n",
    "Y = np.load(\"training_data_Y.pickle\", allow_pickle=True)\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_targets = [\n",
    "#     target_id for recipe in X for target_id in recipe[0]\n",
    "# ]\n",
    "\n",
    "# X_contexts = [\n",
    "#     target_id for recipe in X for target_id in recipe[1]\n",
    "# ]\n",
    "\n",
    "count = 0\n",
    "for recipe in X:\n",
    "    for target_id in recipe[0]:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137483604"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vector size for embeddings\n",
    "embedding_size = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create the model to retrieve the target ingredient's embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 20:47:01.126807: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-08 20:47:01.126837: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-08 20:47:01.126863: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jkunix): /proc/driver/nvidia/version does not exist\n",
      "2023-03-08 20:47:01.127884: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "target_inputs = keras.Input(shape=(1,))\n",
    "\n",
    "target_x = Embedding(\n",
    "    # size of input vector - equal to vocab size\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=embedding_size,\n",
    "    # distribution to sample random values from for initial embeddings\n",
    "    embeddings_initializer=\"glorot_uniform\",\n",
    "    input_length=1\n",
    ")(target_inputs)\n",
    "target_output = Reshape((embedding_size, ))(target_x)\n",
    "\n",
    "target_model = keras.Model(inputs=target_inputs, outputs=target_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create the context ingredient's model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_inputs = keras.Input(shape=(1,))\n",
    "\n",
    "context_x = Embedding(\n",
    "    # size of input vector - equal to vocab size\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=embedding_size,\n",
    "    # distribution to sample random values from for initial embeddings\n",
    "    embeddings_initializer=\"glorot_uniform\",\n",
    "    input_length=1\n",
    ")(context_inputs)\n",
    "context_output = Reshape((embedding_size, ))(context_x)\n",
    "\n",
    "context_model = keras.Model(inputs=context_inputs, outputs=context_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now combine the models together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the dot product of the outputs of the target and context models\n",
    "dot_layer = Dot(axes=1, normalize=False)([target_model.output, context_model.output])\n",
    "\n",
    "# pass the dot product to a dense layer\n",
    "combined_out = Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\")(dot_layer)\n",
    "\n",
    "# compile to a model\n",
    "combined_model = keras.Model(inputs=[target_model.input, context_model.input], outputs=combined_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 1, 100)       735900      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 1, 100)       735900      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 100)          0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 100)          0           ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 1)            0           ['reshape[0][0]',                \n",
      "                                                                  'reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1)            2           ['dot[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,471,802\n",
      "Trainable params: 1,471,802\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "combined_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 205659/511626 [43:17<1:00:53, 83.74it/s]2023-03-08 18:57:06.122464: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "100%|██████████| 511626/511626 [1:44:52<00:00, 81.31it/s]  \n"
     ]
    }
   ],
   "source": [
    "for i in tqdm.tqdm(range(len(X))):\n",
    "    recipe_X = [\n",
    "        np.array(X[i][0],dtype=\"int32\"),\n",
    "        np.array(X[i][1],dtype=\"int32\")\n",
    "    ]\n",
    "    recipe_Y = np.array(Y[i],dtype=\"int32\")\n",
    "    combined_model.train_on_batch(\n",
    "        recipe_X,\n",
    "        recipe_Y\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 18ms/step - loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0292c5a6d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_tmp = [np.array([1,2,4,5],dtype=\"int32\"), np.array([5,6,7,8],dtype=\"int32\")]\n",
    "# y_tmp = np.array([5,2,7,9])\n",
    "\n",
    "# combined_model.fit(X_tmp,\n",
    "#     y_tmp,\n",
    "#     batch_size=1,\n",
    "#     epochs=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredient_layer = combined_model.layers[3].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"weights.npy\",ingredient_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00490222, -0.01940766, -0.01026326,  0.02755293,  0.01797247,\n",
       "        0.00541672,  0.0281049 ,  0.01816593, -0.00531654, -0.02647945,\n",
       "        0.00934397,  0.01540985, -0.01195652,  0.02025651,  0.01323678,\n",
       "        0.00050395, -0.01363507, -0.0105191 , -0.00162502, -0.01784335,\n",
       "       -0.01431565, -0.01396179,  0.0056959 ,  0.00654939, -0.02760641,\n",
       "       -0.01432824,  0.02294516, -0.0192068 ,  0.0250996 , -0.01646177,\n",
       "       -0.01655961, -0.00422387,  0.02272253, -0.00218097, -0.00265214,\n",
       "        0.00980739, -0.01880799,  0.00310035,  0.00465292, -0.00456946,\n",
       "       -0.00567238,  0.00936046, -0.00989753, -0.02441131, -0.00279154,\n",
       "        0.02689251,  0.00120045,  0.00708014,  0.00061905, -0.00800823,\n",
       "        0.00364976, -0.01847172, -0.00255303,  0.01344508,  0.02808063,\n",
       "       -0.01439202, -0.01804162, -0.00081389,  0.01297373, -0.02634827,\n",
       "       -0.00425972,  0.0213232 ,  0.02707435, -0.02066739,  0.0175138 ,\n",
       "        0.00163177,  0.00017825,  0.01611352, -0.01804187,  0.00739604,\n",
       "        0.01335197,  0.00674439, -0.01777561, -0.01402072,  0.01109277,\n",
       "       -0.02006719, -0.01886804, -0.01709086,  0.02785553,  0.0243808 ,\n",
       "        0.01835546,  0.01723435,  0.01300593, -0.00486594, -0.00834166,\n",
       "        0.0255482 , -0.02581636, -0.02696913, -0.01433409,  0.01274922,\n",
       "       -0.01763602, -0.0232425 , -0.02813558, -0.00101924, -0.0171041 ,\n",
       "        0.02729469, -0.01182892,  0.00080174,  0.01920738,  0.02612586],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredient_layer[0][7000]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [25], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m target_inp \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(embedding_size,))\n\u001b[1;32m     26\u001b[0m context_inp \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(embedding_size,))\n\u001b[0;32m---> 28\u001b[0m model\u001b[39m.\u001b[39madd(Dot(axes\u001b[39m=\u001b[39;49m(embedding_size,embedding_size),normalize\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)([target_model, context_model]))\n\u001b[1;32m     29\u001b[0m model\u001b[39m.\u001b[39madd(Dense(\u001b[39m1\u001b[39m, kernel_initializer\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mglorot_uniform\u001b[39m\u001b[39m\"\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     30\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmean_squared_error\u001b[39m\u001b[39m\"\u001b[39m,optimizer\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrmsprop\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/diss/.venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/diss/.venv/lib/python3.8/site-packages/keras/layers/merging/dot.py:119\u001b[0m, in \u001b[0;36mDot.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39m@tf_utils\u001b[39m\u001b[39m.\u001b[39mshape_type_conversion\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild\u001b[39m(\u001b[39mself\u001b[39m, input_shape):\n\u001b[1;32m    118\u001b[0m     \u001b[39m# Used purely for shape validation.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(input_shape[\u001b[39m0\u001b[39;49m], \u001b[39mtuple\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(input_shape) \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    120\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    121\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA `Dot` layer should be called on a list of 2 inputs. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived: input_shape=\u001b[39m\u001b[39m{\u001b[39;00minput_shape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         )\n\u001b[1;32m    124\u001b[0m     shape1 \u001b[39m=\u001b[39m input_shape[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "target_model = Sequential()\n",
    "target_model.add(Embedding(\n",
    "    # size of input vector - equal to vocab size\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=embedding_size,\n",
    "    # distribution to sapmle random values from for initial embeddings\n",
    "    embeddings_initializer=\"glorot_uniform\",\n",
    "    input_length=1\n",
    "))\n",
    "target_model.add(Reshape((embedding_size,)))\n",
    "\n",
    "context_model = Sequential()\n",
    "context_model.add(Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=embedding_size,\n",
    "    embeddings_initializer=\"glorot_uniform\",\n",
    "    input_length=1\n",
    "))\n",
    "context_model.add(Reshape((embedding_size,)))\n",
    "\n",
    "model = Sequential()\n",
    "# combine two embeddings with dot product\n",
    "\n",
    "# context embedding input \n",
    "target_inp = keras.Input(shape=(embedding_size,))\n",
    "context_inp = keras.Input(shape=(embedding_size,))\n",
    "\n",
    "model.add(Dot(axes=(embedding_size,embedding_size),normalize=False)([target_model, context_model]))\n",
    "model.add(Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
    "model.compile(loss=\"mean_squared_error\",optimizer=\"rmsprop\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # length equal to number of recipes\n",
    "# skipgrams = [\n",
    "#     # each row is a recipe\n",
    "#     [\n",
    "#         # first index contains word pairs of (target, context) word\n",
    "#         [\n",
    "#             (target_word_id, context_word_id), ...,    \n",
    "#         ], \n",
    "#         # second index contains labels for the context word\n",
    "#         [\n",
    "#             0, 1, 0, 0, 0, ...\n",
    "#         ]\n",
    "#     ]\n",
    "# ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3, 5), (2, 4, 6)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a single recipe\n",
    "\n",
    "# get a list of all target words\n",
    "\n",
    "# get a list of all context words\n",
    "\n",
    "# get a list of labels for context words\n",
    "\n",
    "# let [list_of_targets, list_of_contexts] be features, X\n",
    "\n",
    "# let labels be Y\n",
    "\n",
    "# fit on X and Y\n",
    "\n",
    "# spread the pairs\n",
    "list(zip(*[(1,2),(3,4),(5,6)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "207bdc028fedaad8767a304f590a2512d39e4178ad48704b78cff486d4be5a5c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
