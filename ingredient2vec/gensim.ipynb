{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import ast\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to parse strings of lists as Python lists\n",
    "def parseTupleFunc(tuple_str: str):\n",
    "\n",
    "    try:\n",
    "        return ast.literal_eval(tuple_str)\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(tuple_str)\n",
    "\n",
    "# import column\n",
    "recipes = pd.read_csv(\"../data/recipes.csv\", usecols=[\"RecipeIngredientParts\"]).squeeze(\"columns\")\n",
    "\n",
    "recipes = recipes.drop(recipes[recipes.str[:2] != \"c(\"].index)\n",
    "\n",
    "recipes = recipes.str[1:]\n",
    "\n",
    "recipes = recipes.apply(parseTupleFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes = recipes.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:24:10: collecting all words and their counts\n",
      "INFO - 22:24:10: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 22:24:10: PROGRESS: at sentence #10000, processed 78783 words, keeping 3498 word types\n",
      "INFO - 22:24:10: PROGRESS: at sentence #20000, processed 156666 words, keeping 4106 word types\n",
      "INFO - 22:24:10: PROGRESS: at sentence #30000, processed 234118 words, keeping 4459 word types\n",
      "INFO - 22:24:10: PROGRESS: at sentence #40000, processed 312517 words, keeping 4694 word types\n",
      "INFO - 22:24:10: PROGRESS: at sentence #50000, processed 392108 words, keeping 4858 word types\n",
      "INFO - 22:24:10: PROGRESS: at sentence #60000, processed 471699 words, keeping 5007 word types\n",
      "INFO - 22:24:10: PROGRESS: at sentence #70000, processed 550299 words, keeping 5126 word types\n",
      "INFO - 22:24:10: PROGRESS: at sentence #80000, processed 631287 words, keeping 5227 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #90000, processed 712934 words, keeping 5395 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #100000, processed 793311 words, keeping 5573 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #110000, processed 875780 words, keeping 5721 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #120000, processed 953574 words, keeping 5787 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #130000, processed 1034017 words, keeping 5842 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #140000, processed 1115188 words, keeping 5906 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #150000, processed 1193851 words, keeping 5967 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #160000, processed 1273109 words, keeping 6032 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #170000, processed 1351713 words, keeping 6073 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #180000, processed 1432251 words, keeping 6126 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #190000, processed 1513299 words, keeping 6169 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #200000, processed 1594181 words, keeping 6212 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #210000, processed 1673262 words, keeping 6262 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #220000, processed 1753135 words, keeping 6310 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #230000, processed 1831365 words, keeping 6358 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #240000, processed 1909358 words, keeping 6390 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #250000, processed 1988304 words, keeping 6433 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #260000, processed 2066882 words, keeping 6464 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #270000, processed 2146775 words, keeping 6495 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #280000, processed 2225474 words, keeping 6523 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #290000, processed 2302840 words, keeping 6554 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #300000, processed 2381100 words, keeping 6583 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #310000, processed 2459562 words, keeping 6612 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #320000, processed 2540749 words, keeping 6646 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #330000, processed 2620439 words, keeping 6670 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #340000, processed 2701943 words, keeping 6703 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #350000, processed 2782900 words, keeping 6733 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #360000, processed 2863945 words, keeping 6791 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #370000, processed 2945972 words, keeping 6827 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #380000, processed 3027762 words, keeping 6857 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #390000, processed 3110306 words, keeping 6899 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #400000, processed 3192242 words, keeping 6939 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #410000, processed 3272235 words, keeping 6977 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #420000, processed 3356013 words, keeping 7007 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #430000, processed 3437614 words, keeping 7036 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #440000, processed 3518441 words, keeping 7071 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #450000, processed 3602971 words, keeping 7114 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #460000, processed 3686573 words, keeping 7179 word types\n",
      "INFO - 22:24:11: PROGRESS: at sentence #470000, processed 3770651 words, keeping 7216 word types\n",
      "INFO - 22:24:12: PROGRESS: at sentence #480000, processed 3855474 words, keeping 7256 word types\n",
      "INFO - 22:24:12: PROGRESS: at sentence #490000, processed 3936148 words, keeping 7296 word types\n",
      "INFO - 22:24:12: PROGRESS: at sentence #500000, processed 4018371 words, keeping 7305 word types\n",
      "INFO - 22:24:12: PROGRESS: at sentence #510000, processed 4106690 words, keeping 7358 word types\n",
      "INFO - 22:24:12: collected 7358 word types from a corpus of 4120761 raw words and 511626 sentences\n",
      "INFO - 22:24:12: Creating a fresh vocabulary\n",
      "INFO - 22:24:12: Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 4749 unique words (64.54% of original 7358, drops 2609)', 'datetime': '2023-03-14T22:24:12.093258', 'gensim': '4.3.1', 'python': '3.8.3 (default, Jul  2 2020, 16:21:59) \\n[GCC 7.3.0]', 'platform': 'Linux-5.15.0-67-generic-x86_64-with-glibc2.10', 'event': 'prepare_vocab'}\n",
      "INFO - 22:24:12: Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 4116453 word corpus (99.90% of original 4120761, drops 4308)', 'datetime': '2023-03-14T22:24:12.094071', 'gensim': '4.3.1', 'python': '3.8.3 (default, Jul  2 2020, 16:21:59) \\n[GCC 7.3.0]', 'platform': 'Linux-5.15.0-67-generic-x86_64-with-glibc2.10', 'event': 'prepare_vocab'}\n",
      "INFO - 22:24:12: deleting the raw counts dictionary of 7358 items\n",
      "INFO - 22:24:12: sample=0.001 downsamples 67 most-common words\n",
      "INFO - 22:24:12: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 2917990.0308687915 word corpus (70.9%% of prior 4116453)', 'datetime': '2023-03-14T22:24:12.126257', 'gensim': '4.3.1', 'python': '3.8.3 (default, Jul  2 2020, 16:21:59) \\n[GCC 7.3.0]', 'platform': 'Linux-5.15.0-67-generic-x86_64-with-glibc2.10', 'event': 'prepare_vocab'}\n",
      "INFO - 22:24:12: estimated required memory for 4749 words and 100 dimensions: 6173700 bytes\n",
      "INFO - 22:24:12: resetting layer weights\n",
      "INFO - 22:24:12: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-03-14T22:24:12.187722', 'gensim': '4.3.1', 'python': '3.8.3 (default, Jul  2 2020, 16:21:59) \\n[GCC 7.3.0]', 'platform': 'Linux-5.15.0-67-generic-x86_64-with-glibc2.10', 'event': 'build_vocab'}\n",
      "INFO - 22:24:12: Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4749 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-03-14T22:24:12.191240', 'gensim': '4.3.1', 'python': '3.8.3 (default, Jul  2 2020, 16:21:59) \\n[GCC 7.3.0]', 'platform': 'Linux-5.15.0-67-generic-x86_64-with-glibc2.10', 'event': 'train'}\n",
      "INFO - 22:24:13: EPOCH 0 - PROGRESS: at 23.10% examples, 640274 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:24:14: EPOCH 0 - PROGRESS: at 49.63% examples, 697372 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:24:15: EPOCH 0 - PROGRESS: at 75.01% examples, 710025 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:24:16: EPOCH 0 - PROGRESS: at 96.30% examples, 692885 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:24:16: EPOCH 0: training on 4120761 raw words (2918148 effective words) took 4.2s, 694659 effective words/s\n",
      "INFO - 22:24:17: EPOCH 1 - PROGRESS: at 26.49% examples, 744199 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:24:18: EPOCH 1 - PROGRESS: at 48.88% examples, 693885 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:24:19: EPOCH 1 - PROGRESS: at 74.05% examples, 703477 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:24:20: EPOCH 1 - PROGRESS: at 99.08% examples, 715030 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:24:20: EPOCH 1: training on 4120761 raw words (2918802 effective words) took 4.1s, 716887 effective words/s\n",
      "INFO - 22:24:21: EPOCH 2 - PROGRESS: at 26.98% examples, 758425 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:24:22: EPOCH 2 - PROGRESS: at 53.80% examples, 758650 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:24:23: EPOCH 2 - PROGRESS: at 76.19% examples, 724123 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:24:24: EPOCH 2 - PROGRESS: at 98.21% examples, 709616 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:24:24: EPOCH 2: training on 4120761 raw words (2917865 effective words) took 4.1s, 711418 effective words/s\n",
      "INFO - 22:24:25: EPOCH 3 - PROGRESS: at 20.18% examples, 567660 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:24:26: EPOCH 3 - PROGRESS: at 43.14% examples, 614137 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:24:27: EPOCH 3 - PROGRESS: at 69.02% examples, 657336 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:24:28: EPOCH 3 - PROGRESS: at 92.74% examples, 670409 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:24:28: EPOCH 3: training on 4120761 raw words (2918303 effective words) took 4.4s, 670247 effective words/s\n",
      "INFO - 22:24:29: EPOCH 4 - PROGRESS: at 23.60% examples, 664369 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:24:30: EPOCH 4 - PROGRESS: at 42.18% examples, 593995 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:24:31: EPOCH 4 - PROGRESS: at 64.46% examples, 608020 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:24:32: EPOCH 4 - PROGRESS: at 81.44% examples, 582515 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:24:34: EPOCH 4 - PROGRESS: at 98.40% examples, 569623 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:24:34: EPOCH 4: training on 4120761 raw words (2917757 effective words) took 5.1s, 568799 effective words/s\n",
      "INFO - 22:24:34: Word2Vec lifecycle event {'msg': 'training on 20603805 raw words (14590875 effective words) took 21.9s, 665964 effective words/s', 'datetime': '2023-03-14T22:24:34.101997', 'gensim': '4.3.1', 'python': '3.8.3 (default, Jul  2 2020, 16:21:59) \\n[GCC 7.3.0]', 'platform': 'Linux-5.15.0-67-generic-x86_64-with-glibc2.10', 'event': 'train'}\n",
      "INFO - 22:24:34: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4749, vector_size=100, alpha=0.025>', 'datetime': '2023-03-14T22:24:34.102991', 'gensim': '4.3.1', 'python': '3.8.3 (default, Jul  2 2020, 16:21:59) \\n[GCC 7.3.0]', 'platform': 'Linux-5.15.0-67-generic-x86_64-with-glibc2.10', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(\n",
    "    recipes,\n",
    "    sg=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:24:41: collecting all words and their counts\n",
      "INFO - 22:24:41: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 22:24:41: PROGRESS: at sentence #10000, processed 78783 words, keeping 3498 word types\n",
      "INFO - 22:24:41: PROGRESS: at sentence #20000, processed 156666 words, keeping 4106 word types\n",
      "INFO - 22:24:41: PROGRESS: at sentence #30000, processed 234118 words, keeping 4459 word types\n",
      "INFO - 22:24:41: PROGRESS: at sentence #40000, processed 312517 words, keeping 4694 word types\n",
      "INFO - 22:24:41: PROGRESS: at sentence #50000, processed 392108 words, keeping 4858 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #60000, processed 471699 words, keeping 5007 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #70000, processed 550299 words, keeping 5126 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #80000, processed 631287 words, keeping 5227 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #90000, processed 712934 words, keeping 5395 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #100000, processed 793311 words, keeping 5573 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #110000, processed 875780 words, keeping 5721 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #120000, processed 953574 words, keeping 5787 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #130000, processed 1034017 words, keeping 5842 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #140000, processed 1115188 words, keeping 5906 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #150000, processed 1193851 words, keeping 5967 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #160000, processed 1273109 words, keeping 6032 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #170000, processed 1351713 words, keeping 6073 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #180000, processed 1432251 words, keeping 6126 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #190000, processed 1513299 words, keeping 6169 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #200000, processed 1594181 words, keeping 6212 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #210000, processed 1673262 words, keeping 6262 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #220000, processed 1753135 words, keeping 6310 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #230000, processed 1831365 words, keeping 6358 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #240000, processed 1909358 words, keeping 6390 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #250000, processed 1988304 words, keeping 6433 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #260000, processed 2066882 words, keeping 6464 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #270000, processed 2146775 words, keeping 6495 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #280000, processed 2225474 words, keeping 6523 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #290000, processed 2302840 words, keeping 6554 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #300000, processed 2381100 words, keeping 6583 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #310000, processed 2459562 words, keeping 6612 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #320000, processed 2540749 words, keeping 6646 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #330000, processed 2620439 words, keeping 6670 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #340000, processed 2701943 words, keeping 6703 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #350000, processed 2782900 words, keeping 6733 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #360000, processed 2863945 words, keeping 6791 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #370000, processed 2945972 words, keeping 6827 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #380000, processed 3027762 words, keeping 6857 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #390000, processed 3110306 words, keeping 6899 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #400000, processed 3192242 words, keeping 6939 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #410000, processed 3272235 words, keeping 6977 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #420000, processed 3356013 words, keeping 7007 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #430000, processed 3437614 words, keeping 7036 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #440000, processed 3518441 words, keeping 7071 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #450000, processed 3602971 words, keeping 7114 word types\n",
      "INFO - 22:24:42: PROGRESS: at sentence #460000, processed 3686573 words, keeping 7179 word types\n",
      "INFO - 22:24:43: PROGRESS: at sentence #470000, processed 3770651 words, keeping 7216 word types\n",
      "INFO - 22:24:43: PROGRESS: at sentence #480000, processed 3855474 words, keeping 7256 word types\n",
      "INFO - 22:24:43: PROGRESS: at sentence #490000, processed 3936148 words, keeping 7296 word types\n",
      "INFO - 22:24:43: PROGRESS: at sentence #500000, processed 4018371 words, keeping 7305 word types\n",
      "INFO - 22:24:43: PROGRESS: at sentence #510000, processed 4106690 words, keeping 7358 word types\n",
      "INFO - 22:24:43: collected 7358 word types from a corpus of 4120761 raw words and 511626 sentences\n",
      "INFO - 22:24:43: Creating a fresh vocabulary\n",
      "INFO - 22:24:43: Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 4749 unique words (64.54% of original 7358, drops 2609)', 'datetime': '2023-03-14T22:24:43.132513', 'gensim': '4.3.1', 'python': '3.8.3 (default, Jul  2 2020, 16:21:59) \\n[GCC 7.3.0]', 'platform': 'Linux-5.15.0-67-generic-x86_64-with-glibc2.10', 'event': 'prepare_vocab'}\n",
      "INFO - 22:24:43: Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 4116453 word corpus (99.90% of original 4120761, drops 4308)', 'datetime': '2023-03-14T22:24:43.135672', 'gensim': '4.3.1', 'python': '3.8.3 (default, Jul  2 2020, 16:21:59) \\n[GCC 7.3.0]', 'platform': 'Linux-5.15.0-67-generic-x86_64-with-glibc2.10', 'event': 'prepare_vocab'}\n",
      "INFO - 22:24:43: deleting the raw counts dictionary of 7358 items\n",
      "INFO - 22:24:43: sample=0.001 downsamples 67 most-common words\n",
      "INFO - 22:24:43: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 2917990.0308687915 word corpus (70.9%% of prior 4116453)', 'datetime': '2023-03-14T22:24:43.179959', 'gensim': '4.3.1', 'python': '3.8.3 (default, Jul  2 2020, 16:21:59) \\n[GCC 7.3.0]', 'platform': 'Linux-5.15.0-67-generic-x86_64-with-glibc2.10', 'event': 'prepare_vocab'}\n",
      "WARNING - 22:24:43: sorting after vectors have been allocated is expensive & error-prone\n",
      "INFO - 22:24:43: estimated required memory for 4749 words and 100 dimensions: 6173700 bytes\n",
      "INFO - 22:24:43: resetting layer weights\n",
      "INFO - 22:24:43: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-03-14T22:24:43.247481', 'gensim': '4.3.1', 'python': '3.8.3 (default, Jul  2 2020, 16:21:59) \\n[GCC 7.3.0]', 'platform': 'Linux-5.15.0-67-generic-x86_64-with-glibc2.10', 'event': 'build_vocab'}\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab(recipes, progress_per=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 22:25:30: Effective 'alpha' higher than previous training cycles\n",
      "INFO - 22:25:30: Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4749 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-03-14T22:25:30.255091', 'gensim': '4.3.1', 'python': '3.8.3 (default, Jul  2 2020, 16:21:59) \\n[GCC 7.3.0]', 'platform': 'Linux-5.15.0-67-generic-x86_64-with-glibc2.10', 'event': 'train'}\n",
      "INFO - 22:25:31: EPOCH 0 - PROGRESS: at 18.71% examples, 520589 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:25:32: EPOCH 0 - PROGRESS: at 43.90% examples, 617972 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:25:33: EPOCH 0 - PROGRESS: at 68.55% examples, 648227 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:25:34: EPOCH 0 - PROGRESS: at 92.74% examples, 667571 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:25:34: EPOCH 0: training on 4120761 raw words (2918146 effective words) took 4.3s, 672240 effective words/s\n",
      "INFO - 22:25:35: EPOCH 1 - PROGRESS: at 26.49% examples, 741462 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:25:36: EPOCH 1 - PROGRESS: at 53.08% examples, 751822 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:25:37: EPOCH 1 - PROGRESS: at 78.10% examples, 741394 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:25:38: EPOCH 1: training on 4120761 raw words (2919199 effective words) took 4.0s, 736747 effective words/s\n",
      "INFO - 22:25:39: EPOCH 2 - PROGRESS: at 24.58% examples, 691406 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:25:40: EPOCH 2 - PROGRESS: at 51.13% examples, 724202 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:25:41: EPOCH 2 - PROGRESS: at 76.66% examples, 729909 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:25:42: EPOCH 2 - PROGRESS: at 99.55% examples, 721896 words/s, in_qsize 2, out_qsize 1\n",
      "INFO - 22:25:42: EPOCH 2: training on 4120761 raw words (2918861 effective words) took 4.0s, 722952 effective words/s\n",
      "INFO - 22:25:43: EPOCH 3 - PROGRESS: at 26.26% examples, 739847 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:25:44: EPOCH 3 - PROGRESS: at 51.13% examples, 723590 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:25:45: EPOCH 3 - PROGRESS: at 75.95% examples, 723230 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:25:46: EPOCH 3 - PROGRESS: at 98.40% examples, 713243 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:25:46: EPOCH 3: training on 4120761 raw words (2918954 effective words) took 4.1s, 714762 effective words/s\n",
      "INFO - 22:25:47: EPOCH 4 - PROGRESS: at 23.85% examples, 666432 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:25:48: EPOCH 4 - PROGRESS: at 45.40% examples, 644148 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:25:49: EPOCH 4 - PROGRESS: at 66.62% examples, 632342 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:25:50: EPOCH 4 - PROGRESS: at 86.92% examples, 624327 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:25:51: EPOCH 4: training on 4120761 raw words (2917451 effective words) took 4.7s, 620208 effective words/s\n",
      "INFO - 22:25:52: EPOCH 5 - PROGRESS: at 22.61% examples, 629425 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:25:53: EPOCH 5 - PROGRESS: at 43.64% examples, 612665 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:25:54: EPOCH 5 - PROGRESS: at 61.52% examples, 578348 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:25:55: EPOCH 5 - PROGRESS: at 80.05% examples, 569898 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:25:56: EPOCH 5 - PROGRESS: at 99.08% examples, 572321 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:25:56: EPOCH 5: training on 4120761 raw words (2918235 effective words) took 5.1s, 572458 effective words/s\n",
      "INFO - 22:25:57: EPOCH 6 - PROGRESS: at 21.35% examples, 597571 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:25:58: EPOCH 6 - PROGRESS: at 41.94% examples, 594326 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:25:59: EPOCH 6 - PROGRESS: at 63.47% examples, 601026 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:00: EPOCH 6 - PROGRESS: at 83.83% examples, 601606 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:01: EPOCH 6: training on 4120761 raw words (2918453 effective words) took 4.9s, 601492 effective words/s\n",
      "INFO - 22:26:02: EPOCH 7 - PROGRESS: at 21.35% examples, 595878 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:03: EPOCH 7 - PROGRESS: at 42.67% examples, 602364 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:26:04: EPOCH 7 - PROGRESS: at 64.93% examples, 611410 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:05: EPOCH 7 - PROGRESS: at 85.77% examples, 610567 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:06: EPOCH 7: training on 4120761 raw words (2918192 effective words) took 4.8s, 610250 effective words/s\n",
      "INFO - 22:26:07: EPOCH 8 - PROGRESS: at 22.09% examples, 615837 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:08: EPOCH 8 - PROGRESS: at 44.15% examples, 621460 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:26:09: EPOCH 8 - PROGRESS: at 65.65% examples, 619638 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:26:10: EPOCH 8 - PROGRESS: at 86.46% examples, 619035 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:10: EPOCH 8: training on 4120761 raw words (2917681 effective words) took 4.8s, 610718 effective words/s\n",
      "INFO - 22:26:12: EPOCH 9 - PROGRESS: at 20.66% examples, 571173 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:26:13: EPOCH 9 - PROGRESS: at 42.18% examples, 590739 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:26:14: EPOCH 9 - PROGRESS: at 64.46% examples, 605924 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:26:15: EPOCH 9 - PROGRESS: at 83.11% examples, 591750 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:15: EPOCH 9: training on 4120761 raw words (2917005 effective words) took 5.0s, 588350 effective words/s\n",
      "INFO - 22:26:16: EPOCH 10 - PROGRESS: at 21.35% examples, 595402 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:17: EPOCH 10 - PROGRESS: at 40.94% examples, 578158 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:18: EPOCH 10 - PROGRESS: at 56.84% examples, 537302 words/s, in_qsize 4, out_qsize 1\n",
      "INFO - 22:26:19: EPOCH 10 - PROGRESS: at 73.57% examples, 524167 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:20: EPOCH 10 - PROGRESS: at 87.85% examples, 504651 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:21: EPOCH 10: training on 4120761 raw words (2917673 effective words) took 5.8s, 503279 effective words/s\n",
      "INFO - 22:26:22: EPOCH 11 - PROGRESS: at 15.12% examples, 415056 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:23: EPOCH 11 - PROGRESS: at 33.41% examples, 469148 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:24: EPOCH 11 - PROGRESS: at 50.38% examples, 474597 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:26:25: EPOCH 11 - PROGRESS: at 68.78% examples, 488081 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:26: EPOCH 11 - PROGRESS: at 88.56% examples, 507882 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:26:27: EPOCH 11: training on 4120761 raw words (2918246 effective words) took 5.6s, 518755 effective words/s\n",
      "INFO - 22:26:28: EPOCH 12 - PROGRESS: at 21.35% examples, 591775 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:29: EPOCH 12 - PROGRESS: at 43.39% examples, 612453 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:30: EPOCH 12 - PROGRESS: at 65.89% examples, 623178 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:31: EPOCH 12 - PROGRESS: at 87.39% examples, 627642 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:32: EPOCH 12: training on 4120761 raw words (2917055 effective words) took 4.6s, 628253 effective words/s\n",
      "INFO - 22:26:33: EPOCH 13 - PROGRESS: at 22.35% examples, 630364 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:34: EPOCH 13 - PROGRESS: at 44.66% examples, 630840 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:35: EPOCH 13 - PROGRESS: at 66.62% examples, 627980 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:36: EPOCH 13 - PROGRESS: at 87.85% examples, 628960 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:36: EPOCH 13: training on 4120761 raw words (2917916 effective words) took 4.6s, 629197 effective words/s\n",
      "INFO - 22:26:37: EPOCH 14 - PROGRESS: at 22.61% examples, 632129 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:26:38: EPOCH 14 - PROGRESS: at 45.40% examples, 641210 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:39: EPOCH 14 - PROGRESS: at 68.08% examples, 644907 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:26:40: EPOCH 14 - PROGRESS: at 89.73% examples, 645714 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:41: EPOCH 14: training on 4120761 raw words (2917963 effective words) took 4.5s, 642927 effective words/s\n",
      "INFO - 22:26:42: EPOCH 15 - PROGRESS: at 22.61% examples, 633365 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:43: EPOCH 15 - PROGRESS: at 43.90% examples, 623128 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:44: EPOCH 15 - PROGRESS: at 65.65% examples, 618302 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:45: EPOCH 15 - PROGRESS: at 85.77% examples, 611542 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:45: EPOCH 15: training on 4120761 raw words (2918697 effective words) took 4.8s, 614307 effective words/s\n",
      "INFO - 22:26:46: EPOCH 16 - PROGRESS: at 22.61% examples, 636813 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:47: EPOCH 16 - PROGRESS: at 45.15% examples, 641982 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:26:49: EPOCH 16 - PROGRESS: at 67.84% examples, 643402 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:50: EPOCH 16 - PROGRESS: at 89.26% examples, 642210 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:26:50: EPOCH 16: training on 4120761 raw words (2917472 effective words) took 4.5s, 641397 effective words/s\n",
      "INFO - 22:26:51: EPOCH 17 - PROGRESS: at 23.10% examples, 649475 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:26:52: EPOCH 17 - PROGRESS: at 45.91% examples, 648440 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:26:53: EPOCH 17 - PROGRESS: at 68.78% examples, 647735 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:54: EPOCH 17 - PROGRESS: at 90.39% examples, 647593 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:55: EPOCH 17: training on 4120761 raw words (2918263 effective words) took 4.5s, 647574 effective words/s\n",
      "INFO - 22:26:56: EPOCH 18 - PROGRESS: at 23.10% examples, 648678 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:57: EPOCH 18 - PROGRESS: at 45.91% examples, 650725 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:26:58: EPOCH 18 - PROGRESS: at 67.35% examples, 638134 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:26:59: EPOCH 18 - PROGRESS: at 89.02% examples, 640113 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:26:59: EPOCH 18: training on 4120761 raw words (2917960 effective words) took 4.6s, 640712 effective words/s\n",
      "INFO - 22:27:00: EPOCH 19 - PROGRESS: at 22.61% examples, 636271 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:01: EPOCH 19 - PROGRESS: at 45.40% examples, 642497 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:27:02: EPOCH 19 - PROGRESS: at 67.84% examples, 641314 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:27:03: EPOCH 19 - PROGRESS: at 89.26% examples, 640338 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:04: EPOCH 19: training on 4120761 raw words (2917843 effective words) took 4.6s, 640667 effective words/s\n",
      "INFO - 22:27:05: EPOCH 20 - PROGRESS: at 22.85% examples, 643219 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:27:06: EPOCH 20 - PROGRESS: at 45.40% examples, 642753 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:07: EPOCH 20 - PROGRESS: at 66.86% examples, 633915 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:08: EPOCH 20 - PROGRESS: at 88.09% examples, 634118 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:08: EPOCH 20: training on 4120761 raw words (2918543 effective words) took 4.6s, 632921 effective words/s\n",
      "INFO - 22:27:09: EPOCH 21 - PROGRESS: at 22.35% examples, 631261 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:10: EPOCH 21 - PROGRESS: at 43.90% examples, 621677 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:11: EPOCH 21 - PROGRESS: at 66.38% examples, 627716 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:12: EPOCH 21 - PROGRESS: at 87.39% examples, 626476 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:13: EPOCH 21: training on 4120761 raw words (2918046 effective words) took 4.6s, 627667 effective words/s\n",
      "INFO - 22:27:14: EPOCH 22 - PROGRESS: at 21.84% examples, 612527 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:15: EPOCH 22 - PROGRESS: at 43.90% examples, 623087 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:16: EPOCH 22 - PROGRESS: at 66.38% examples, 629532 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:27:17: EPOCH 22 - PROGRESS: at 87.85% examples, 629680 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:27:18: EPOCH 22: training on 4120761 raw words (2916366 effective words) took 4.6s, 627902 effective words/s\n",
      "INFO - 22:27:19: EPOCH 23 - PROGRESS: at 22.35% examples, 630157 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:27:20: EPOCH 23 - PROGRESS: at 43.90% examples, 622195 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:21: EPOCH 23 - PROGRESS: at 65.40% examples, 615480 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:27:22: EPOCH 23 - PROGRESS: at 86.92% examples, 621013 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:22: EPOCH 23: training on 4120761 raw words (2917159 effective words) took 4.7s, 624277 effective words/s\n",
      "INFO - 22:27:23: EPOCH 24 - PROGRESS: at 23.10% examples, 640817 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:24: EPOCH 24 - PROGRESS: at 45.91% examples, 648349 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:25: EPOCH 24 - PROGRESS: at 68.78% examples, 650571 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:26: EPOCH 24 - PROGRESS: at 90.39% examples, 650150 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:27: EPOCH 24: training on 4120761 raw words (2918407 effective words) took 4.5s, 647488 effective words/s\n",
      "INFO - 22:27:28: EPOCH 25 - PROGRESS: at 23.35% examples, 658334 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:27:29: EPOCH 25 - PROGRESS: at 45.91% examples, 651337 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:30: EPOCH 25 - PROGRESS: at 67.59% examples, 640821 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:31: EPOCH 25 - PROGRESS: at 88.56% examples, 636217 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:27:31: EPOCH 25: training on 4120761 raw words (2917927 effective words) took 4.6s, 636810 effective words/s\n",
      "INFO - 22:27:32: EPOCH 26 - PROGRESS: at 23.35% examples, 658740 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:27:33: EPOCH 26 - PROGRESS: at 45.66% examples, 646540 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:34: EPOCH 26 - PROGRESS: at 68.08% examples, 643752 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:35: EPOCH 26 - PROGRESS: at 89.26% examples, 640816 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:27:36: EPOCH 26: training on 4120761 raw words (2918497 effective words) took 4.6s, 640563 effective words/s\n",
      "INFO - 22:27:37: EPOCH 27 - PROGRESS: at 22.85% examples, 643610 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:38: EPOCH 27 - PROGRESS: at 44.90% examples, 638662 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:39: EPOCH 27 - PROGRESS: at 67.84% examples, 641430 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:40: EPOCH 27 - PROGRESS: at 89.02% examples, 639729 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:41: EPOCH 27: training on 4120761 raw words (2919616 effective words) took 4.6s, 637833 effective words/s\n",
      "INFO - 22:27:42: EPOCH 28 - PROGRESS: at 22.61% examples, 636777 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 22:27:43: EPOCH 28 - PROGRESS: at 44.90% examples, 638319 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:44: EPOCH 28 - PROGRESS: at 67.10% examples, 637938 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:45: EPOCH 28 - PROGRESS: at 87.85% examples, 633043 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:45: EPOCH 28: training on 4120761 raw words (2919638 effective words) took 4.6s, 633725 effective words/s\n",
      "INFO - 22:27:46: EPOCH 29 - PROGRESS: at 22.85% examples, 639251 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:47: EPOCH 29 - PROGRESS: at 45.40% examples, 642930 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:48: EPOCH 29 - PROGRESS: at 67.84% examples, 643164 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:49: EPOCH 29 - PROGRESS: at 89.49% examples, 643495 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:27:50: EPOCH 29: training on 4120761 raw words (2918577 effective words) took 4.5s, 643446 effective words/s\n",
      "INFO - 22:27:50: Word2Vec lifecycle event {'msg': 'training on 123622830 raw words (87544041 effective words) took 139.9s, 625559 effective words/s', 'datetime': '2023-03-14T22:27:50.201915', 'gensim': '4.3.1', 'python': '3.8.3 (default, Jul  2 2020, 16:21:59) \\n[GCC 7.3.0]', 'platform': 'Linux-5.15.0-67-generic-x86_64-with-glibc2.10', 'event': 'train'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(87544041, 123622830)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(recipes, total_examples=model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tuna in water', 0.8306821584701538),\n",
       " ('canned tuna', 0.8179494142532349),\n",
       " ('albacore tuna', 0.7582552433013916),\n",
       " ('tuna fish', 0.7243056893348694),\n",
       " ('tuna in vegetable oil', 0.7168256640434265),\n",
       " ('light chunk tuna in water', 0.6869235634803772),\n",
       " ('salmon', 0.683331310749054),\n",
       " ('solid white tuna packed in water', 0.6772703528404236),\n",
       " ('albacore tuna in water', 0.653946578502655),\n",
       " ('chunk light tuna', 0.6511041522026062)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"tuna\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "207bdc028fedaad8767a304f590a2512d39e4178ad48704b78cff486d4be5a5c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
