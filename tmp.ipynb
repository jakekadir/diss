{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recipe_rec.data_loader import get_recipes\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded (511626, 28) recipes...\n",
      "Encoding ingredients to SBERT vectors:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a658da97e9415da3670b9d2a88ec81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bert_encoder = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "# load the full dataset\n",
    "full_dataset = get_recipes(\"./data/recipes.csv\")\n",
    "\n",
    "print(f\"Loaded {full_dataset.shape} recipes...\")\n",
    "\n",
    "# turn ingredients back into a string\n",
    "full_dataset[\"RecipeIngredientParts\"] = full_dataset[\"RecipeIngredientParts\"].str.join(\n",
    "    \" \"\n",
    ")\n",
    "\n",
    "print(\"Encoding ingredients to SBERT vectors:\")\n",
    "\n",
    "encoded_ingredients = bert_encoder.encode(\n",
    "    full_dataset[\"RecipeIngredientParts\"].values, show_progress_bar=True\n",
    ")\n",
    "with open(\"sbert_ingredient_vectors.pkl\", \"wb\") as f:\n",
    "\n",
    "    pickle.dump(encoded_ingredients, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jake/diss/.venv/lib/python3.8/site-packages/numpy/core/getlimits.py:500: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/jake/diss/.venv/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/home/jake/diss/.venv/lib/python3.8/site-packages/numpy/core/getlimits.py:500: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/jake/diss/.venv/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    }
   ],
   "source": [
    "from glove import Glove, Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import pprint\n",
    "import gensim\n",
    "\n",
    "from glove import Glove\n",
    "from glove import Corpus\n",
    "\n",
    "\n",
    "def read_corpus(filename):\n",
    "\n",
    "    delchars = [chr(c) for c in range(256)]\n",
    "    delchars = [x for x in delchars if not x.isalnum()]\n",
    "    delchars.remove(\" \")\n",
    "    delchars = \"\".join(delchars)\n",
    "\n",
    "    print(delchars)\n",
    "    # with open(filename, 'r') as datafile:\n",
    "    #     for line in datafile:\n",
    "    #         yield line.lower().translate(None, delchars).split(' ')\n",
    "\n",
    "\n",
    "def read_wikipedia_corpus(filename):\n",
    "\n",
    "    # We don't want to do a dictionary construction pass.\n",
    "    corpus = gensim.corpora.WikiCorpus(filename, dictionary={})\n",
    "\n",
    "    for text in corpus.get_texts():\n",
    "        yield text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\u0001\u0002\u0003\u0004\u0005\u0006\t\n",
      "\u000e\u000f\u0010\u0011\u0012\u0013\u0014\u0015\u0016\u0017\u0018\u0019\u001a\u001b\u001c\u001d\u001e\u001f!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ ¡¢£¤¥¦§¨©«¬­®¯°±´¶·¸»¿×÷\n"
     ]
    }
   ],
   "source": [
    "read_corpus(\"akjhd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up command line parameters.\n",
    "# parser = argparse.ArgumentParser(description='Fit a GloVe model.')\n",
    "\n",
    "# parser.add_argument('--create', '-c', action='store',\n",
    "#                     default=None,\n",
    "#                     help=('The filename of the corpus to pre-process. '\n",
    "#                             'The pre-processed corpus will be saved '\n",
    "#                             'and will be ready for training.'))\n",
    "# parser.add_argument('-wiki', '-w', action='store_true',\n",
    "#                     default=False,\n",
    "#                     help=('Assume the corpus input file is in the '\n",
    "#                             'Wikipedia dump format'))\n",
    "# parser.add_argument('--train', '-t', action='store',\n",
    "#                     default=0,\n",
    "#                     help=('Train the GloVe model with this number of epochs.'\n",
    "#                             'If not supplied, '\n",
    "#                             'We\\'ll attempt to load a trained model'))\n",
    "# parser.add_argument('--parallelism', '-p', action='store',\n",
    "#                     default=1,\n",
    "#                     help=('Number of parallel threads to use for training'))\n",
    "# parser.add_argument('--query', '-q', action='store',\n",
    "#                     default='',\n",
    "#                     help='Get closes words to this word.')\n",
    "\n",
    "# args = {}\n",
    "\n",
    "# if args.create:\n",
    "#     # Build the corpus dictionary and the cooccurrence matrix.\n",
    "#     print('Pre-processing corpus')\n",
    "\n",
    "#     if args.wiki:\n",
    "#         print('Using wikipedia corpus')\n",
    "#         get_data = read_wikipedia_corpus\n",
    "#     else:\n",
    "#         get_data = read_corpus\n",
    "\n",
    "#     corpus_model = Corpus()\n",
    "#     corpus_model.fit(get_data(args.create), window=10)\n",
    "#     corpus_model.save('corpus.model')\n",
    "\n",
    "#     print('Dict size: %s' % len(corpus_model.dictionary))\n",
    "#     print('Collocations: %s' % corpus_model.matrix.nnz)\n",
    "\n",
    "# if args.train:\n",
    "#     # Train the GloVe model and save it to disk.\n",
    "\n",
    "#     if not args.create:\n",
    "#         # Try to load a corpus from disk.\n",
    "#         print('Reading corpus statistics')\n",
    "#         corpus_model = Corpus.load('corpus.model')\n",
    "\n",
    "#         print('Dict size: %s' % len(corpus_model.dictionary))\n",
    "#         print('Collocations: %s' % corpus_model.matrix.nnz)\n",
    "\n",
    "#     print('Training the GloVe model')\n",
    "\n",
    "#     glove = Glove(no_components=100, learning_rate=0.05)\n",
    "#     glove.fit(corpus_model.matrix, epochs=int(args.train),\n",
    "#                 no_threads=args.parallelism, verbose=True)\n",
    "#     glove.add_dictionary(corpus_model.dictionary)\n",
    "\n",
    "#     glove.save('glove.model')\n",
    "\n",
    "# if args.query:\n",
    "#     # Finally, query the model for most similar words.\n",
    "#     if not args.train:\n",
    "#         print('Loading pre-trained GloVe model')\n",
    "#         glove = Glove.load('glove.model')\n",
    "\n",
    "#     print('Querying for %s' % args.query)\n",
    "\n",
    "#     pprint.pprint(glove.most_similar(args.query, number=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recipe_rec.data_loader import get_recipes\n",
    "\n",
    "df = get_recipes(\"./data/recipes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_ingredients = df[\"RecipeIngredientParts\"].apply(lambda tup: list(tup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 25 training epochs with 2 threads\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n"
     ]
    }
   ],
   "source": [
    "VECTOR_SIZE = 100\n",
    "NUM_EPOCHS = 25\n",
    "LEARNING_RATE = 0.001\n",
    "WINDOW = 5\n",
    "\n",
    "corpus = Corpus()\n",
    "corpus.fit(recipe_ingredients, window=WINDOW)\n",
    "corpus.save(\"corpus.model\")\n",
    "\n",
    "\n",
    "glove = Glove(no_components=VECTOR_SIZE, learning_rate=LEARNING_RATE)\n",
    "glove.fit(corpus.matrix, epochs=NUM_EPOCHS, verbose=True)\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "glove.save(\"glove.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cinnamon', 0.9975804099733946),\n",
       " ('all-purpose flour', 0.9971611296325676),\n",
       " ('vanilla extract', 0.9971607104886437),\n",
       " ('egg', 0.9968816220333483)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.most_similar(\"granulated sugar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
